---
id: mem-20260119-d4e5f6
question: "How is the protocol configuration handled in `llm-env` provider settings?"
created: 2026-01-19
last_retrieved: ""
sprints:
  - "1.0_anthropic_protocol_support"
files: []
tags:
  - configuration
  - protocol
  - backward-compatibility
  - validation
retrievals: 0
status: active
---

# Provider Protocol Configuration Pattern

## Decision

The `llm-env` configuration introduces an optional `protocol` field for each provider. This field implements the following logic:
1.  **Default Value:** It defaults to `openai` if not specified, ensuring backward compatibility with existing configurations.
2.  **Validation:** It validates the input against a strict whitelist (currently `openai` and `anthropic`).
3.  **Normalization:** It normalizes user input by converting to lowercase and trimming whitespace.
4.  **Fallback:** If an invalid protocol is provided, it falls back to the default `openai` protocol and issues a warning message.

## Rationale

-   **Backward Compatibility:** Ensures that users with existing configuration files do not need to make changes to continue using the system.
-   **Controlled Extension:** The whitelist allows for the safe addition of new protocols without risking breakage from unrecognized values.
-   **Robustness:** Normalization and fallback mechanisms handle user input errors or variations gracefully, improving user experience.

## Applies When

-   Defining or parsing provider configurations in `llm-env`.
-   Adding support for new LLM providers that might use different communication protocols.
-   Validating user-supplied configuration values.

## Code Reference

N/A
