# LLM Environment Manager Configuration
# This file defines available providers, their API endpoints, and default models
# Users can customize this file or create their own in ~/.config/llm-env/config.conf
#
# Format:
# [provider_name]
# base_url=https://api.example.com/v1
# api_key_var=LLM_EXAMPLE_API_KEY
# default_model=model-name
# description=Optional description
# enabled=true|false

# ============================================================================
# Major LLM Providers
# ============================================================================

[openai]
base_url=https://api.openai.com/v1
api_key_var=LLM_OPENAI_API_KEY
default_model=gpt-5
description=Industry standard GPT models, highest quality
enabled=true

[gemini]
base_url=https://generativelanguage.googleapis.com/v1beta/openai
api_key_var=LLM_GEMINI_API_KEY
default_model=gemini-1.5-flash
description=Google Gemini models with native OpenAI compatibility
enabled=false

# ============================================================================
# High-Speed Providers
# ============================================================================

[groq]
base_url=https://api.groq.com/openai/v1
api_key_var=LLM_GROQ_API_KEY
default_model=openai/gpt-oss-120b
description=Lightning-fast inference, great for real-time applications
enabled=true

[groq2]
base_url=https://api.groq.com/openai/v1
api_key_var=LLM_GROQ_API_KEY
default_model=qwen/qwen3-32b
description=Alternative Groq configuration with Qwen model
enabled=true

# ============================================================================
# OpenRouter (Multiple Models)
# ============================================================================

[openrouter]
base_url=https://openrouter.ai/api/v1
api_key_var=LLM_OPENROUTER_API_KEY
default_model=x-ai/grok-code-fast-1
description=Access to multiple models, coding-focused
enabled=true

[openrouter2]
base_url=https://openrouter.ai/api/v1
api_key_var=LLM_OPENROUTER_API_KEY
default_model=deepseek/deepseek-chat-v3.1:free
description=Free tier option, great for testing
enabled=true

[openrouter3]
base_url=https://openrouter.ai/api/v1
api_key_var=LLM_OPENROUTER_API_KEY
default_model=qwen/qwen3-coder:free
description=Free coding model, good for development
enabled=true

# ============================================================================
# Local/Self-Hosted Options
# ============================================================================

[ollama]
base_url=http://localhost:11434/v1
api_key_var=OLLAMA_API_KEY
default_model=local-model
description=Local Ollama instance, privacy-focused
enabled=false

[lmstudio]
base_url=http://localhost:1234/v1
api_key_var=LMSTUDIO_API_KEY
default_model=local-model
description=LM Studio local server
enabled=false

[vllm]
base_url=http://localhost:8000/v1
api_key_var=VLLM_API_KEY
default_model=local-model
description=vLLM local deployment
enabled=false

# ============================================================================
# Alternative Providers
# ============================================================================

[cerebras]
base_url=https://api.cerebras.ai/v1
api_key_var=LLM_CEREBRAS_API_KEY
default_model=qwen-3-coder-480b
description=Fast inference with competitive pricing, great for coding
enabled=true

[together]
base_url=https://api.together.xyz/v1
api_key_var=LLM_TOGETHER_API_KEY
default_model=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
description=Together AI, competitive pricing with wide model selection
enabled=false

[fireworks]
base_url=https://api.fireworks.ai/inference/v1
api_key_var=LLM_FIREWORKS_API_KEY
default_model=accounts/fireworks/models/llama-v3p1-405b-instruct
description=Fireworks AI, ultra-fast inference optimized for production
enabled=false

[perplexity]
base_url=https://api.perplexity.ai
api_key_var=LLM_PERPLEXITY_API_KEY
default_model=llama-3.1-sonar-large-128k-online
description=Perplexity AI, real-time web search and reasoning capabilities
enabled=false

[replicate]
base_url=https://api.replicate.com/v1
api_key_var=LLM_REPLICATE_API_KEY
default_model=meta/meta-llama-3.1-405b-instruct
description=Replicate, pay-per-use model hosting with diverse options
enabled=false

[xai]
base_url=https://api.x.ai/v1
api_key_var=LLM_XAI_API_KEY
default_model=grok-code-fast-1
description=xAI Grok, excellent reasoning and coding capabilities
enabled=false

[deepinfra]
base_url=https://api.deepinfra.com/v1/openai
api_key_var=LLM_DEEPINFRA_API_KEY
default_model=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
description=DeepInfra, cost-effective inference with popular models
enabled=false

[deepseek]
base_url=https://api.deepseek.com/v1
api_key_var=LLM_DEEPSEEK_API_KEY
default_model=deepseek-chat
description=DeepSeek, excellent coding and reasoning models
enabled=false

[sambanova]
base_url=https://api.sambanova.ai/v1
api_key_var=LLM_SAMBANOVA_API_KEY
default_model=meta-llama/Llama-3.2-3B-Instruct
description=Ultra-fast inference with custom hardware acceleration
enabled=false

[anyscale]
base_url=https://api.anyscale.com/v1
api_key_var=LLM_ANYSCALE_API_KEY
default_model=meta-llama/Meta-Llama-3.1-70B-Instruct
description=Enterprise-grade LLM inference with Ray integration
enabled=false

# Anthropic Claude via proxy (requires gateway like Portkey or LiteLLM)
# [anthropic]
# base_url=https://api.portkey.ai/v1
# api_key_var=LLM_ANTHROPIC_API_KEY
# default_model=claude-3-5-sonnet-20241022
# description=Anthropic Claude via Portkey gateway
# enabled=false

# ============================================================================
# Configuration Notes
# ============================================================================
#
# To add a custom provider:
# 1. Add a new section with [provider_name]
# 2. Set the required fields: base_url, api_key_var, default_model
# 3. Optionally add description and set enabled=true
# 4. Make sure your API key is set in your shell profile
#
# To disable a provider:
# Set enabled=false or comment out the entire section
#
# To override models:
# Set OPENAI_MODEL_OVERRIDE environment variable
#
# Configuration file precedence:
# 1. ~/.config/llm-env/config.conf (user-specific)
# 2. /usr/local/etc/llm-env/config.conf (system-wide)
# 3. Built-in defaults (fallback)
#
# Example custom provider:
# [my_custom_api]
# base_url=https://my-api.example.com/v1
# api_key_var=MY_CUSTOM_API_KEY
# default_model=my-model-v1
# description=My custom LLM API
# enabled=true