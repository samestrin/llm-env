# LLM Environment Manager Configuration
# This file defines available providers, their API endpoints, and default models
# Users can customize this file or create their own in ~/.config/llm-env/config.conf
#
# Format:
# [provider_name]
# base_url=https://api.example.com/v1
# api_key_var=LLM_EXAMPLE_API_KEY
# default_model=model-name
# description=Optional description
# enabled=true|false

# ============================================================================
# Major LLM Providers
# ============================================================================

[cerebras]
base_url=https://api.cerebras.ai/v1
api_key_var=LLM_CEREBRAS_API_KEY
default_model=qwen-3-coder-480b
description=Fast inference with competitive pricing, great for coding
enabled=true

[openai]
base_url=https://api.openai.com/v1
api_key_var=LLM_OPENAI_API_KEY
default_model=gpt-5-2025-08-07
description=Industry standard GPT models, highest quality
enabled=true



# ============================================================================
# High-Speed Providers
# ============================================================================

[groq]
base_url=https://api.groq.com/openai/v1
api_key_var=LLM_GROQ_API_KEY
default_model=openai/gpt-oss-120b
description=Lightning-fast inference, great for real-time applications
enabled=true

[groq2]
base_url=https://api.groq.com/openai/v1
api_key_var=LLM_GROQ_API_KEY
default_model=qwen/qwen3-32b
description=Alternative Groq configuration with Qwen model
enabled=true

# ============================================================================
# OpenRouter (Multiple Models)
# ============================================================================

[openrouter]
base_url=https://openrouter.ai/api/v1
api_key_var=LLM_OPENROUTER_API_KEY
default_model=x-ai/grok-code-fast-1
description=Access to multiple models, coding-focused
enabled=true

[openrouter2]
base_url=https://openrouter.ai/api/v1
api_key_var=LLM_OPENROUTER_API_KEY
default_model=deepseek/deepseek-chat-v3.1:free
description=Free tier option, great for testing
enabled=true

[openrouter3]
base_url=https://openrouter.ai/api/v1
api_key_var=LLM_OPENROUTER_API_KEY
default_model=qwen/qwen3-coder:free
description=Free coding model, good for development
enabled=true

# ============================================================================
# Local/Self-Hosted Options
# ============================================================================

[ollama]
base_url=http://localhost:11434/v1
api_key_var=OLLAMA_API_KEY
default_model=qwen2.5-coder:7b
description=Local Ollama instance, privacy-focused
enabled=false

[lmstudio]
base_url=http://localhost:1234/v1
api_key_var=LMSTUDIO_API_KEY
default_model=local-model
description=LM Studio local server
enabled=false

[vllm]
base_url=http://localhost:8000/v1
api_key_var=VLLM_API_KEY
default_model=meta-llama/Llama-3.1-8B-Instruct
description=vLLM local deployment
enabled=false

# ============================================================================
# Alternative Providers
# ============================================================================

[together]
base_url=https://api.together.xyz/v1
api_key_var=LLM_TOGETHER_API_KEY
default_model=meta-llama/Llama-3.1-70B-Instruct-Turbo
description=Together AI, good performance and pricing
enabled=false

[fireworks]
base_url=https://api.fireworks.ai/inference/v1
api_key_var=LLM_FIREWORKS_API_KEY
default_model=accounts/fireworks/models/llama-v3p1-70b-instruct
description=Fireworks AI, fast inference
enabled=false

[perplexity]
base_url=https://api.perplexity.ai
api_key_var=LLM_PERPLEXITY_API_KEY
default_model=llama-3.1-sonar-large-128k-online
description=Perplexity AI with web search capabilities
enabled=false

[replicate]
base_url=https://api.replicate.com/v1
api_key_var=LLM_REPLICATE_API_KEY
default_model=meta/llama-2-70b-chat
description=Replicate platform, various models
enabled=false

# ============================================================================
# Configuration Notes
# ============================================================================
#
# To add a custom provider:
# 1. Add a new section with [provider_name]
# 2. Set the required fields: base_url, api_key_var, default_model
# 3. Optionally add description and set enabled=true
# 4. Make sure your API key is set in your shell profile
#
# To disable a provider:
# Set enabled=false or comment out the entire section
#
# To override models:
# Set OPENAI_MODEL_OVERRIDE environment variable
#
# Configuration file precedence:
# 1. ~/.config/llm-env/config.conf (user-specific)
# 2. /usr/local/etc/llm-env/config.conf (system-wide)
# 3. Built-in defaults (fallback)
#
# Example custom provider:
# [my_custom_api]
# base_url=https://my-api.example.com/v1
# api_key_var=MY_CUSTOM_API_KEY
# default_model=my-model-v1
# description=My custom LLM API
# enabled=true